{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96061636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Constants.Paths import *\n",
    "from Constants.Labels import *\n",
    "from Unpacking.PrepareAudioFiles import prepare_audio_files\n",
    "from Preprocessing.GenerateSpectrograms import generate_all_spectrograms\n",
    "from SpectrogramLoading import *\n",
    "from Models.TrainingHistory import TrainingHistory\n",
    "from Models.CnnModel import CnnModel\n",
    "from Models.TransformerModel import TransformerModel\n",
    "from Models.WandbDetails import WandbDetails\n",
    "from Models.InputPadding import pad_to_length\n",
    "from Models.HistoryPlots import plot_loss_history, plot_accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ddc4cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silence folder /home/jknyspel/Documents/Code/DeepLearningSpeechRecognition/Dataset/train/audio/silence already exists. Skipping.\n",
      "Already extended. Skipping.\n",
      "Output directory /home/jknyspel/Documents/Code/DeepLearningSpeechRecognition/Dataset/train/spectrograms already exists. Skipping.\n",
      "Output directory /home/jknyspel/Documents/Code/DeepLearningSpeechRecognition/Dataset/test/spectrograms already exists. Skipping.\n"
     ]
    }
   ],
   "source": [
    "prepare_audio_files()\n",
    "generate_all_spectrograms(backend=\"soundfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e99b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, val_paths = get_divided_paths_with_labels()\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_paths)\n",
    "random.shuffle(val_paths)\n",
    "\n",
    "train = [load_spectrogram_for_path(path_with_label) for path_with_label in train_paths]\n",
    "validation = [load_spectrogram_for_path(path_with_label) for path_with_label in val_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f416762",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, label_indexes = spectrograms_to_x_y(train)\n",
    "X_validation, y_validation, _ = spectrograms_to_x_y(validation, label_indexes)\n",
    "\n",
    "max_length = max(x.shape[1] for x in [*X_validation, *X_train])\n",
    "X_validation = pad_to_length(X_validation, max_length)\n",
    "X_train = pad_to_length(X_train, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c437aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{classifier_dropout_1=0.0042, classifier_dropout_2=0.1834, classifier_dropout_3=0.2591, eps=8.4356e-06, inv_beta_1=0.0764, inv_beta_2=1.5100e-04, learning_rate=1.7422e-04, lr_decay=1.5753e-05} -> 91.10412\n",
      "{classifier_dropout_1=0.1364, classifier_dropout_2=0.2498, classifier_dropout_3=0.0207, eps=3.9558e-07, inv_beta_1=0.0062, inv_beta_2=3.0996e-05, learning_rate=4.3379e-06, lr_decay=4.6450e-06} -> 48.38293\n",
      "\n",
      "Best parameters: classifier_dropout_1=0.0042, classifier_dropout_2=0.1834, classifier_dropout_3=0.2591, eps=8.4356e-06, inv_beta_1=0.0764, inv_beta_2=1.5100e-04, learning_rate=1.7422e-04, lr_decay=1.5753e-05\n",
      "Best score: 91.10412\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import optimization\n",
    "from skopt.space import Real\n",
    "\n",
    "\n",
    "def evaluate_cnn_params(epochs: int, **params: dict[str, Any]) -> float:\n",
    "    cnn_model = CnnModel(\n",
    "        classes=labels,\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        lr_decay=params[\"lr_decay\"],\n",
    "        beta_1=1-params[\"inv_beta_1\"],\n",
    "        beta_2=1-params[\"inv_beta_2\"],\n",
    "        eps=params[\"eps\"],\n",
    "        classifier_dropout_1=params[\"classifier_dropout_1\"],\n",
    "        classifier_dropout_2=params[\"classifier_dropout_2\"],\n",
    "        classifier_dropout_3=params[\"classifier_dropout_3\"],\n",
    "        print_every=None, # Disable printing epoch info\n",
    "        validate_every=epochs, # Validate only on last epoch\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    cnn_model.train((X_train, y_train), (X_validation, y_validation), epochs=epochs, batch_size=32)\n",
    "    return cnn_model.get_history().val_accuracy[-1] \n",
    "\n",
    "cnn_spaces = {\n",
    "    \"learning_rate\": Real(1e-6, 1e-3, \"log-uniform\"),\n",
    "    \"lr_decay\": Real(1e-6, 1e-2, \"log-uniform\"),\n",
    "    \"inv_beta_1\": Real(1e-4, 0.5, \"log-uniform\"),\n",
    "    \"inv_beta_2\": Real(1e-6, 0.1, \"log-uniform\"),\n",
    "    \"eps\": Real(1e-8, 1e-3, \"log-uniform\"),\n",
    "    \"classifier_dropout_1\": Real(0.0, 0.3, \"uniform\"),\n",
    "    \"classifier_dropout_2\": Real(0.0, 0.3, \"uniform\"),\n",
    "    \"classifier_dropout_3\": Real(0.0, 0.3, \"uniform\")\n",
    "}\n",
    "\n",
    "best_cnn_params_from_search, best_cnn_accuracy = optimization.bayes_search(\n",
    "    lambda **params: evaluate_cnn_params(epochs=50, **params), \n",
    "    cnn_spaces, \n",
    "    iterations=2)\n",
    "\n",
    "best_cnn_params = {\n",
    "    \"learning_rate\": best_cnn_params_from_search[\"learning_rate\"],\n",
    "    \"lr_decay\": best_cnn_params_from_search[\"lr_decay\"],\n",
    "    \"beta_1\": 1 - best_cnn_params_from_search[\"inv_beta_1\"],\n",
    "    \"beta_2\": 1 - best_cnn_params_from_search[\"inv_beta_2\"],\n",
    "    \"eps\": best_cnn_params_from_search[\"eps\"],\n",
    "    \"classifier_dropout_1\": best_cnn_params_from_search[\"classifier_dropout_1\"],\n",
    "    \"classifier_dropout_2\": best_cnn_params_from_search[\"classifier_dropout_2\"],\n",
    "    \"classifier_dropout_3\": best_cnn_params_from_search[\"classifier_dropout_3\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f647a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_coeffs_to_params(**params: dict[str, Any]) -> dict[str, Any]:\n",
    "    coeff_param_names = [\"embedding_dimension\", \"num_attention_heads\", \"num_encoder_layers\", \"dim_feedforward\"]\n",
    "    default_params = {\n",
    "        \"embedding_dimension\": 512,\n",
    "        \"num_attention_heads\": 8,\n",
    "        \"num_encoder_layers\": 6,\n",
    "        \"dim_feedforward\": 2048,\n",
    "    }\n",
    "    \n",
    "    coeff_mean = sum(params[f\"{name}_coeff\"] for name in coeff_param_names) / len(coeff_param_names)\n",
    "    transformed_params = {\n",
    "        param_name: int(params[f\"{param_name}_coeff\"] / coeff_mean * default_params[param_name])\n",
    "        for param_name in coeff_param_names\n",
    "    }\n",
    "    \n",
    "    # Embedding dimension must be even\n",
    "    if transformed_params[\"embedding_dimension\"] % 2 != 0:\n",
    "        transformed_params[\"embedding_dimension\"] += 1\n",
    "    \n",
    "    return transformed_params    \n",
    "\n",
    "def evaluate_transformer_params(epochs: int, **params: dict[str, Any]) -> float:\n",
    "    transformer_model = TransformerModel(\n",
    "        classes=labels,\n",
    "        **transformer_coeffs_to_params(**params),\n",
    "        dropout=params[\"dropout\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        lr_decay=params[\"lr_decay\"],\n",
    "        beta_1=1-params[\"inv_beta_1\"],\n",
    "        beta_2=1-params[\"inv_beta_2\"],\n",
    "        eps=params[\"eps\"],\n",
    "        print_every=None, # Disable printing epoch info\n",
    "        validate_every=epochs, # Validate only on last epoch\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    transformer_model.train((X_train, y_train), (X_validation, y_validation), epochs=epochs, batch_size=8)\n",
    "    return transformer_model.get_history().val_accuracy[-1] \n",
    "\n",
    "transformer_spaces = {\n",
    "    \"embedding_dimension_coeff\": Real(0.1, 0.9, \"uniform\"),\n",
    "    \"num_attention_heads_coeff\": Real(0.1, 0.9, \"uniform\"),\n",
    "    \"num_encoder_layers_coeff\": Real(0.1, 0.9, \"uniform\"),\n",
    "    \"dim_feedforward_coeff\": Real(0.1, 0.9, \"uniform\"),\n",
    "    \"dropout\": Real(0.0, 0.3, \"uniform\"),\n",
    "    \"learning_rate\": Real(1e-5, 1e-3, \"log-uniform\"),\n",
    "    \"lr_decay\": Real(1e-6, 1e-2, \"log-uniform\"),\n",
    "    \"inv_beta_1\": Real(1e-4, 0.5, \"log-uniform\"),\n",
    "    \"inv_beta_2\": Real(1e-6, 0.1, \"log-uniform\"),\n",
    "    \"eps\": Real(1e-8, 1e-3, \"log-uniform\")\n",
    "}\n",
    "\n",
    "best_transformer_params_from_search, best_transformer_accuracy = optimization.bayes_search(\n",
    "    lambda **params: evaluate_transformer_params(epochs=5, **params), \n",
    "    transformer_spaces, \n",
    "    iterations=2)\n",
    "\n",
    "best_transformer_params = {\n",
    "    **transformer_coeffs_to_params(**best_transformer_params_from_search),\n",
    "    \"dropout\": best_transformer_params_from_search[\"dropout\"],\n",
    "    \"learning_rate\": best_transformer_params_from_search[\"learning_rate\"],\n",
    "    \"lr_decay\": best_transformer_params_from_search[\"lr_decay\"],\n",
    "    \"beta_1\": 1 - best_transformer_params_from_search[\"inv_beta_1\"],\n",
    "    \"beta_2\": 1 - best_transformer_params_from_search[\"inv_beta_2\"],\n",
    "    \"eps\": best_transformer_params_from_search[\"eps\"]\n",
    "}\n",
    "\n",
    "# best_transformer_params = {\n",
    "#     \"embedding_dimension\": 512,\n",
    "#     \"num_attention_heads\": 8,\n",
    "#     \"num_encoder_layers\": 6,\n",
    "#     \"dim_feedforward\": 2048,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"learning_rate\": 1e-4,\n",
    "#     \"lr_decay\": 1e-6,\n",
    "#     \"beta_1\": 0.9,\n",
    "#     \"beta_2\": 0.98,\n",
    "#     \"eps\": 1e-9\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f823c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5d30df",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.64 GiB of which 41.25 MiB is free. Process 207127 has 5.97 MiB memory in use. Process 302350 has 4.62 GiB memory in use. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.43 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m transformer_results = []\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m transformer_param_grid:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     result = \u001b[43mtrain_and_evaluate_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     transformer_results.append(result)\n\u001b[32m     88\u001b[39m save_transformer_results_to_json(transformer_results, \u001b[33m\"\u001b[39m\u001b[33mtransformer_experiments_results.json\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_and_evaluate_transformer\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_and_evaluate_transformer\u001b[39m(params):\n\u001b[32m      4\u001b[39m     model = TransformerModel(\n\u001b[32m      5\u001b[39m         classes=labels,\n\u001b[32m      6\u001b[39m         embedding_dimension=params.get(\u001b[33m\"\u001b[39m\u001b[33membedding_dimension\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m512\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m         seed=\u001b[32m42\u001b[39m\n\u001b[32m     19\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     y_pred_classes = model.predict(X_validation)\n\u001b[32m     25\u001b[39m     acc = accuracy_score(y_validation, y_pred_classes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/src/Models/TransformerModel.py:222\u001b[39m, in \u001b[36mTransformerModel.train\u001b[39m\u001b[34m(self, train_data, val_data, epochs, batch_size)\u001b[39m\n\u001b[32m    218\u001b[39m src_key_padding_mask = \u001b[38;5;28mself\u001b[39m._generate_padding_mask(inputs) \u001b[38;5;66;03m# (B, S), True=valid\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass mask\u001b[39;00m\n\u001b[32m    224\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, targets_one_hot)\n\u001b[32m    226\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/src/Models/TransformerModel.py:501\u001b[39m, in \u001b[36mTransformerModel.TransformerModule.forward\u001b[39m\u001b[34m(self, src, src_key_padding_mask)\u001b[39m\n\u001b[32m    498\u001b[39m pytorch_padding_mask = ~src_key_padding_mask \n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# print(f\"Inverted Mask shape: {pytorch_padding_mask.shape}\") # True=ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m encoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpytorch_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# print(f\"Encoder Output shape: {encoder_output.shape}\") # B, S, D\u001b[39;00m\n\u001b[32m    503\u001b[39m \n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# 4. Aggregate sequence output (Mean Pooling over sequence dim)\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# Mask the padded outputs before averaging.\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Expand mask to match encoder_output dimensions (B, S, D)\u001b[39;00m\n\u001b[32m    507\u001b[39m mask_expanded = src_key_padding_mask.unsqueeze(-\u001b[32m1\u001b[39m).expand_as(encoder_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:517\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    514\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    525\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:922\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    918\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    919\u001b[39m         x\n\u001b[32m    920\u001b[39m         + \u001b[38;5;28mself\u001b[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001b[32m    921\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:947\u001b[39m, in \u001b[36mTransformerEncoderLayer._ff_block\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.linear2(\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    948\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/DeepLearningSpeechRecognition/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 7.64 GiB of which 41.25 MiB is free. Process 207127 has 5.97 MiB memory in use. Process 302350 has 4.62 GiB memory in use. Including non-PyTorch memory, this process has 2.60 GiB memory in use. Of the allocated memory 2.43 GiB is allocated by PyTorch, and 3.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_transformer(params):\n",
    "    model = TransformerModel(\n",
    "        classes=labels,\n",
    "        embedding_dimension=params.get(\"embedding_dimension\", 512),\n",
    "        num_attention_heads=params.get(\"num_attention_heads\", 8),\n",
    "        num_encoder_layers=params.get(\"num_encoder_layers\", 6),\n",
    "        dim_feedforward=params.get(\"dim_feedforward\", 2048),\n",
    "        dropout=params.get(\"dropout\", 0.1),\n",
    "        learning_rate=params.get(\"learning_rate\", 1e-4),\n",
    "        lr_decay=params.get(\"lr_decay\", 0.0),\n",
    "        beta_1=params.get(\"beta_1\", 0.9),\n",
    "        beta_2=params.get(\"beta_2\", 0.98),\n",
    "        eps=params.get(\"eps\", 1e-9),\n",
    "        print_every=None,\n",
    "        validate_every=1,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    model.train((X_train, y_train), (X_validation, y_validation), epochs=5, batch_size=32)\n",
    "    \n",
    "    y_pred_classes = model.predict(X_validation)\n",
    "    \n",
    "    acc = accuracy_score(y_validation, y_pred_classes)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_validation, y_pred_classes, average=None, labels=list(range(len(labels)))\n",
    "    )\n",
    "    cm = confusion_matrix(y_validation, y_pred_classes)\n",
    "    history = model.get_history()\n",
    "    \n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"history\": history\n",
    "    }\n",
    "\n",
    "def save_transformer_results_to_json(results, filename):\n",
    "    serializable_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        serializable_result = {\n",
    "            \"params\": result[\"params\"],\n",
    "            \"accuracy\": result[\"accuracy\"],\n",
    "            \"precision\": result[\"precision\"].tolist(),\n",
    "            \"recall\": result[\"recall\"].tolist(),\n",
    "            \"f1_score\": result[\"f1_score\"].tolist(),\n",
    "            \"confusion_matrix\": result[\"confusion_matrix\"].tolist(),\n",
    "            \"history\": {\n",
    "                \"loss\": result[\"history\"].loss,\n",
    "                \"val_loss\": result[\"history\"].val_loss,\n",
    "                \"accuracy\": result[\"history\"].accuracy,\n",
    "                \"val_accuracy\": result[\"history\"].val_accuracy,\n",
    "            }\n",
    "        }\n",
    "        serializable_results.append(serializable_result)\n",
    "    \n",
    "    with open(f\"../Outputs/{filename}\", \"w\") as f:\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "transformer_param_grid = [\n",
    "    {**best_transformer_params, \"embedding_dimension\": 256},\n",
    "    {**best_transformer_params, \"embedding_dimension\": 512},\n",
    "    {**best_transformer_params, \"embedding_dimension\": 1024},\n",
    "    \n",
    "    {**best_transformer_params, \"num_attention_heads\": 4},\n",
    "    {**best_transformer_params, \"num_attention_heads\": 8},\n",
    "    {**best_transformer_params, \"num_attention_heads\": 16},\n",
    "    \n",
    "    {**best_transformer_params, \"dropout\": 0.1},\n",
    "    {**best_transformer_params, \"dropout\": 0.2},\n",
    "    {**best_transformer_params, \"dropout\": 0.3},\n",
    "    \n",
    "    {**best_transformer_params, \"learning_rate\": 1e-3},\n",
    "    {**best_transformer_params, \"learning_rate\": 1e-4},\n",
    "    {**best_transformer_params, \"learning_rate\": 1e-5},\n",
    "]\n",
    "\n",
    "transformer_results = []\n",
    "for params in transformer_param_grid:\n",
    "    result = train_and_evaluate_transformer(params)\n",
    "    transformer_results.append(result)\n",
    "\n",
    "save_transformer_results_to_json(transformer_results, \"transformer_experiments_results.json\")\n",
    "\n",
    "for i, result in enumerate(transformer_results):\n",
    "    print(f\"\\nModel {i+1}\")\n",
    "    print(f\"Params: {result['params']}\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']}\")\n",
    "    print(f\"Recall: {result['recall']}\")\n",
    "    print(f\"F1-Score: {result['f1_score']}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(result[\"confusion_matrix\"], annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'Confusion Matrix for Model {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c29f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing CNN\n",
    "\n",
    "def train_and_evaluate_cnn(params):\n",
    "    model = CnnModel(\n",
    "        classes=labels,\n",
    "        learning_rate=params.get(\"learning_rate\", 1e-3),\n",
    "        lr_decay=params.get(\"lr_decay\", 1e-4),\n",
    "        beta_1=params.get(\"beta_1\", 0.9),\n",
    "        beta_2=params.get(\"beta_2\", 0.999),\n",
    "        eps=params.get(\"eps\", 1e-8),\n",
    "        classifier_dropout_1=params.get(\"classifier_dropout_1\", 0.3),\n",
    "        classifier_dropout_2=params.get(\"classifier_dropout_2\", 0.3),\n",
    "        classifier_dropout_3=params.get(\"classifier_dropout_3\", 0.1),\n",
    "        print_every=None,\n",
    "        validate_every=1,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    model.train((X_train, y_train), (X_validation, y_validation), epochs=5, batch_size=32)\n",
    "    \n",
    "    y_pred_classes = model.predict(X_validation)\n",
    "    \n",
    "    acc = accuracy_score(y_validation, y_pred_classes)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        y_validation, y_pred_classes, average=None, labels=list(range(len(labels)))\n",
    "    )\n",
    "    cm = confusion_matrix(y_validation, y_pred_classes)\n",
    "    history = model.get_history()\n",
    "    \n",
    "    return {\n",
    "        \"params\": params,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"history\": history\n",
    "    }\n",
    "\n",
    "def save_cnn_results_to_json(results, filename):\n",
    "    serializable_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        serializable_result = {\n",
    "            \"params\": result[\"params\"],\n",
    "            \"accuracy\": result[\"accuracy\"],\n",
    "            \"precision\": result[\"precision\"].tolist(),\n",
    "            \"recall\": result[\"recall\"].tolist(),\n",
    "            \"f1_score\": result[\"f1_score\"].tolist(),\n",
    "            \"confusion_matrix\": result[\"confusion_matrix\"].tolist(),\n",
    "            \"history\": {\n",
    "                \"loss\": result[\"history\"].loss,\n",
    "                \"val_loss\": result[\"history\"].val_loss,\n",
    "                \"accuracy\": result[\"history\"].accuracy,\n",
    "                \"val_accuracy\": result[\"history\"].val_accuracy,\n",
    "            }\n",
    "        }\n",
    "        serializable_results.append(serializable_result)\n",
    "    \n",
    "    with open(f\"../Outputs/{filename}\", \"w\") as f:\n",
    "        json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "cnn_param_grid = [\n",
    "    {**best_cnn_params, \"classifier_dropout_1\": 0.2, \"classifier_dropout_2\": 0.2, \"classifier_dropout_3\": 0.1},\n",
    "    {**best_cnn_params, \"classifier_dropout_1\": 0.3, \"classifier_dropout_2\": 0.3, \"classifier_dropout_3\": 0.1},\n",
    "    {**best_cnn_params, \"classifier_dropout_1\": 0.4, \"classifier_dropout_2\": 0.4, \"classifier_dropout_3\": 0.2},\n",
    "    \n",
    "    {**best_cnn_params, \"learning_rate\": 1e-3},\n",
    "    {**best_cnn_params, \"learning_rate\": 1e-4},\n",
    "    {**best_cnn_params, \"learning_rate\": 5e-5},\n",
    "]\n",
    "\n",
    "cnn_results = []\n",
    "\n",
    "for params in cnn_param_grid:\n",
    "    result = train_and_evaluate_cnn(params)\n",
    "    cnn_results.append(result)\n",
    "\n",
    "save_cnn_results_to_json(cnn_results, \"cnn_experiments_results.json\")\n",
    "\n",
    "for i, result in enumerate(cnn_results):\n",
    "    print(f\"\\nModel {i+1}\")\n",
    "    print(f\"Params: {result['params']}\")\n",
    "    print(f\"Accuracy: {result['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']}\")\n",
    "    print(f\"Recall: {result['recall']}\")\n",
    "    print(f\"F1-Score: {result['f1_score']}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(result[\"confusion_matrix\"], annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(f'Confusion Matrix for Model {i+1}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
